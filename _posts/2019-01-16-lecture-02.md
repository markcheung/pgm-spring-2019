---
layout: distill
title: "Lecture 02: Bayesian Networks"
description: TBD
date: 2019-01-16

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Cathy Su
    url: "https://ceesu.github.io"   
  - name: Mark Cheung
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

***

# In class notes

Motivation: representing joint distributions over some random variables is computationally expensive, so we need methodologies to represent joint distributions compactly.

## Specification of a Directed Graphical Model

There are two components to any GM:

* qualitative (topology)
* quantitative (numbers associated with each conditional distributions)

### Sources of Qualitative Specifications

Where do our assumptions come from?
* Prior knowledge of causal relationship
* Prior knowledge of modular relationship
* Assessment from expert
* Learning from data
* We simply like a certain architecture (e.g. a layered graph)
* ...

## Local Structures & Independencies

* Common parent
  * Fixing B decouples A and C
  * "given the level of gene B, the levels of A and C are independent"
<img src="{{ '/assets/img/notes/lecture-02/common_parent.png' | relative_url }}" />

$$ A \perp C |B => P(A,C|B)=P(A|B)P(C|B) $$

* Cascade
  * Knowing B decouples A and C
  * "given the level of gene B, the level gene A provides no extra prediction value for the level of gene C"
<img src="{{ '/assets/img/notes/lecture-02/cascade.png' | relative_url }}" />

* V-structure
  * Knowing C couples A and B because A can "explain away" B w.r.t. C
  * "If A correlates to C, then chance for B to also correlate to B will decrease"
  
<img src="{{ '/assets/img/notes/lecture-02/V-structure.png' | relative_url }}" />

  In class example ($$ A $$ and $$ B $$ become coupled if $$ C $$ is known because they jointly cause the 3rd event): $$  P(A,B)=P(A)P(B) $$ but $$ P (A,B \vert C) $$ cannot be decoupled into two conditionals
  
* The language is compact, the concepts are rich!

### A simple justification

Prove: $$ A \perp C \vert B => P(A,C \vert B)=P(A \vert B)P(C \vert B) $$

From the motif: $$P(A,B,C)=P(B)P(A \vert B)P(C \vert B)$$

$$P(A,C \vert B)=P(A,B,C)/P(B)$$

Plugging in the above,  $$P(A,C \vert B)=P(B)P(A \vert B)P(C \vert B)/P(B)=P(A \vert B)P(C \vert B)$$

## I-maps

* Defn: Let $$P$$ be a distribution over $$X$$. We define $$I(P)$$ to be the set of independence assertions of the form $$(X \perp Y \vert Z)$$ that hold in $$P$$ (however how we set the parameter-values).

* Defn: Let $$K$$ be any graph object associated with a set of independencies $$I(K)$$. We say that $$K$$ is an I-map for a set of independencies $$I$$, if $$I(K) \subseteq I$$.

* We now say that $$G$$ is an I-map for $$P$$ if $$G$$ is an I-map for $$I(P)$$, where we use $$I(G)$$ as the set of independencies associated.

## Facts about I-map

* For $$G$$ to be an I-map of $$P$$, it is necessary that $$G$$ does not mislead us regarding independencies in $$P$$:

  any independence that $$G$$ asserts must also hold in $$P$$. Conversely, $$P$$ may have additional independencies that are not reflected in $$G$$
 
  Example:
  
* $$ X \perp Y $$

<img src="{{ '/assets/img/notes/lecture-02/I-Map_a.png' | relative_url }}" />

* $$ \emptyset$$

<img src="{{ '/assets/img/notes/lecture-02/I-Map_b.png' | relative_url }}" />

* $$ \emptyset$$

<img src="{{ '/assets/img/notes/lecture-02/I-Map_c.png' | relative_url }}" />

* $$P_1$$: $$ I(P_1) = {X \perp Y} $$ (from inspection i.e., $$P(X,Y)=P(X)P(Y), 0.48=0.6*0.8$$)

<img src="{{ '/assets/img/notes/lecture-02/I-Map_p1.png' | relative_url }}" />

This is the first graph.

* $$P_2$$: $$I(P_2) = \emptyset $$

<img src="{{ '/assets/img/notes/lecture-02/I-Map_p2.png' | relative_url }}" />

Both the middle and the last graph since they assume no independences. Therefore, they are equivalent in terms of independent sets. The first one has an independent set. Therefore it is not an I-map of $$P_2$$

## What is $$I(G)$$

local Markov assumptions of BN

  A *Bayesian network structure* $$G$$ is a directed acyclic graph whose nodes represent random variables $$X_1, . . . ,X_n$$
  
  local Markov assumptions
* Defn:

Let $$Pa_{X_i}$$ denote the parents of $$X_i$$ in $$G$$, and $$ NonDescendants_{X_i} $$ denote the variables in the graph that are not descendants of $$X_i$$. Then $$G$$ encodes the following set of local conditional independence assumptions $$I_{\ell}(G)$$:

$$ I_{\ell} (G): \left\{ X_i \perp NonDescendants_{X_i} \vert Pa_{X_i}: \forall i \right\} $$

In other words, each node $$X_i$$ is independent of its nondescendants given its parents.

## Graph Separation Criterion

* D-separation criterion for Bayesian networks (D for Directed edges):

* Defn: variables x and y are *D-separated* (conditionally independent) given z if they are separated in the *moralized* ancestral graph

* Example: $$ X \perp Y \vert Z$$, then we say $$Z$$ D-separates $$X$$ and $$Y$$.

<img src="{{ '/assets/img/notes/lecture-02/graph_separation_criterion_example.png' | relative_url }}" width="500" />

  * Ancestral graph (focusing only the nodes of interest): keeping only the nodes themselves + ancestors of the nodes in question.

  * Moral ancestral: remove the connectivity between nodes, but connect parents to nodes that are shared by a single child.

If there is a way to travel from one node to another node using any path (not thru the given), then these two nodes are not conditionally independent (example is not conditionally independent).


## What is in $$I(G)$$

Global Markov properties of Bayesian networks
* X is *d-separated* (directed-separated) from Z given Y if we can't send a ball from any node in X to any node in Z using the *"Bayes-ball"* algorithm illustrated bellow (and plus some boundary conditions):

<img src="{{ '/assets/img/notes/lecture-02/d_separated.png' | relative_url }}" />

  * Defn: $$I(G) =$$ all independence properties that correspond to d-separation:
  
  $$ I(G): \left\{ X_i \perp Z \vert Y: dsep_G ( X ; Z \vert Y) \right\} $$
  
  * D-separation is sound and complete
  
