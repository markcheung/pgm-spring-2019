---
layout: distill
title: "Lecture 02: Bayesian Networks"
description: TBD
date: 2019-01-16

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Cathy Su
    url: "https://ceesu.github.io"   
  - name: Mark Cheung
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

***

# Readings
## The Bayesian network representation <d-cite key="Koller:2009:PGM:1795555"></d-cite>

### 3.1.1  Exploiting independence Properties
#### Standard vs compact parametrization of  independent random variables
Given random variables $$X_i$$ each representing the outcome of an independent coin toss, the standard parametrization of their joint distribution would be as follows:

$$ P(X_1, ... X_n) = P(X_1 = x_1, X_2=x_2, ..., X_n = x_n) = P(X_1 = x_1)P(X_2=x_2)...P(X_n=x_n) $$

Note there are 2 possibilities for each outcome $$x_i$$, this representation requires $$2^n$$ total number of parameters. 

One simple way to reduce the number of parameters needed, would be to represent the probability that each coin toss lands heads as $$ \theta_1, ... \theta_n $$.  Then we have the following compact representation requiring only $$n$$ parameters:

$$ P(X_1, ... X_n) = \prod_i \theta_{X_i} $$

### 3.1.3 Naive Bayes
We can further express the joint distribution in terms of conditional probabilities. This is done in the Naive Bayes model.

 Instances of this model have will include:
* class : some category $$C \in \{c^1, ...c^k\}$$ which gives a prior on the value of each feature
* features: observed properties $$X_1, ... X_k$$ 

The model makes the strong  'naive' conditional independence assumption:

$$ P(x_i | C,  x_1, \dots, x_n) = P(x_i | C) $$

In words, _features are conditionally independent, given the class of the instance of this model_.  Thus the joint distribution of the Naive Bayes model factorizes as follows:

$$ P(C, X_1, ... X_n) = P(C) \prod_i P(X_i |C)$$

### 3.2.1 Bayesian networks
Unlike in the Naive Bayes model, Bayesian networks can also represent distributions that do not satisfy the naive conditional independence assumption. 

The Bayesian network is represented by a directed acyclic graph $$\mathcal{G}$$  whose nodes represent random variables $$X_1, ...X_n $$., which serves to represent the joint distribution compactly, by representing a set of conditional probability assumptions about the distribution. 

Specifically (Definition )

### 3.2.3 Graphs and Distributions

This section includes  $$P$$ satisfies the local independencies associated with  $$\mathcal{G} \iff P$$ is representable as a set of conditional probability distributions (CPDs) associated with $$\mathcal{G}$$. 

#### I-Map

### Box 3.C Knowledge engineering

 Building a Bayesian Network in the real world requires many steps including:

* _picking variables precisely_: we need some variables that can be observed, and their domain should be specified. Sometimes introducing hidden variables will help to render the observed variables conditionally independent.
* _picking a structure consistent with the causal order_: choose enough variables to approximate the causal relationships. 
* _picking probability estimates for events in the network_: data collection may be difficult but small errors have little effect (though the network is sensitive to events assigned a zero probability, and to relative size of probabilities of events)

Done correctly, the model will be accurate as well as not too complex to use (see Box 3.D).

### 3.3.1 D-separation

Objective: determine which independencies hold for every distribution $$P$$ which factorizes over $$G$$.

By examining 2-edge connections between nodes $$X$$, $$Y$$ and $$Z$$

Further, "directed separation" (d-separation) 

#### Definition 3.6 active trail
Let $$\mathcal{G}$$ be a BN structure and. $$ X_1 <=> , ... <=> X_n  $$ be a trail in $$\mathcal{G}$$. Let $$Z$$
Be a subset of observed variables. Then the trail $$ X_1 <=> , ... <=> X_n  $$ is active given $$Z  $$ if:

* Whenever we have.a v-structure $$ X_{I-1}  \to X_i \leftarrow X_{I+1}  $$ then $$X_i$$ or one of its descendants are in $$Z$$
* No other node along the trail is in $$Z$$

#### Definition 3.7 D-separation 


### 3.3.2 Soundness and Completeness

*Soundness:* if $$ X, Y $$ are d-separated given $$ Z, \implies X \perp Y | Z $$.  i.e. independence determined by d-separation is satisfied by the underlying distribution. 

*Completeness:*  If two variables $$ X \perp Y | Z $$ then they are d-separated. 

For proof, please see the textbook.

Test image

<img src="{{ '/assets/img/notes/lecture-02/cmu-logo.png' | relative_url }}" />

***

# In class notes

Motivation: representing joint distributions over some random variables is computationally expensive, so we need methodologies to represent joint distributions compactly.

## Specification of a Directed Graphical Model

There are two components to any GM:

* qualitative (topology)
* quantitative (numbers associated with each conditional distributions)

### Sources of Qualitative Specifications

Where do our assumptions come from?
* Prior knowledge of causal relationship
* Prior knowledge of modular relationship
* Assessment from expert
* Learning from data
* We simply like a certain architecture (e.g. a layered graph)
* ...

## Local Structures & Independencies

* Common parent
  * Fixing B decouples A and C
  * "given the level of gene B, the levels of A and C are independent"
<img src="{{ '/assets/img/notes/lecture-02/common_parent.png' | relative_url }}" />

$$ A \perp C |B => P(A,C|B)=P(A|B)P(C|B) $$

* Cascade
  * Knowing B decouples A and C
  * "given the level of gene B, the level gene A provides no extra prediction value for the level of gene C"
<img src="{{ '/assets/img/notes/lecture-02/cascade.png' | relative_url }}" />

* V-structure
  * Knowing C couples A and B because A can "explain away" B w.r.t. C
  * "If A correlates to C, then chance for B to also correlate to B will decrease"
  
<img src="{{ '/assets/img/notes/lecture-02/V-structure.png' | relative_url }}" />

  In class example ($$ A $$ and $$ B $$ become coupled if $$ C $$ is known because they jointly cause the 3rd event): $$  P(A,B)=P(A)P(B) $$ but $$ P (A,B \vert C) $$ cannot be decoupled into two conditionals
  
* The language is compact, the concepts are rich!

### A simple justification

Prove: $$ A \perp C \vert B => P(A,C \vert B)=P(A \vertB)P(C \vertB) $$

From the motif: $$P(A,B,C)=P(B)P(A \vertB)P(C \vertB)$$

$$P(A,C \vert B)=P(A,B,C)/P(B)$$

Plugging in the above,  $$P(A,C \vert B)=P(B)P(A \vert B)P(C \vert B)/P(B)=P(A \vert B)P(C \vert B)$$

## I-maps

* Defn: Let $$P$$ be a distribution over $$X$$. We define $$I(P)$$ to be the set of independence assertions of the form $$(X \perp Y \vert Z)$$ that hold in $$P$$ (however how we set the parameter-values).

* Defn: Let $$K$$ be any graph object associated with a set of independencies $$I(K)$$. We say that $$K$$ is an I-map for a set of independencies $$I$$, if $$I(K) \subseteq I$$.

* We now say that $$G$$ is an I-map for $$P$$ if $$G$$ is an I-map for $$I(P)$$, where we use $$I(G)$$ as the set of independencies associated.
